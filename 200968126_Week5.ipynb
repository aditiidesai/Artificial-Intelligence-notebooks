{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#AI WEEK 5\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Aditi Desai*\n",
        "\n",
        "*200968126*"
      ],
      "metadata": {
        "id": "gZz0dOpe8a4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " MULTI ARMED BANDITS IN TF-AGENTS"
      ],
      "metadata": {
        "id": "tCn5G3Fq8n0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Armed Bandit (MAB) is a Machine Learning framework in which an agent has to select actions (arms) in order to maximize its cumulative reward in the long term. In each round, the agent receives some information about the current state (context), then it chooses an action based on this information and the experience gathered in previous rounds. At the end of each round, the agent receives the reward associated with the chosen action."
      ],
      "metadata": {
        "id": "yUqB6QnH8o6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtQu2qmO555C",
        "outputId": "fc1f2f5a-0425-4d0b-c1cc-c0eee0a31b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697660 sha256=5e095c54ddf020adb19521a5127fcb68b87e2ed8c84cde77997af1fe04b6b629\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\n",
            "Successfully built gym\n",
            "Installing collected packages: pygame, gym, tf-agents\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "metadata": {
        "id": "sybg6svY5_ma"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1 -Create a environment"
      ],
      "metadata": {
        "id": "vpJ-WDKN6L9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.for  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation."
      ],
      "metadata": {
        "id": "1ZLMQ9jh6Muo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiArmedBanditEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._episode_ended = False\n",
        "    self._observation = None\n",
        "    self._reward = None\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._observation = np.random.randint(low=-5, high=6)\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    self._reward = self._observation * action\n",
        "    self._episode_ended = True\n",
        "    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)"
      ],
      "metadata": {
        "id": "Fox2_PuI6WQf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive."
      ],
      "metadata": {
        "id": "VXc7yivt6aYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_policy(observation):\n",
        "  if observation < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "z_xy36LZ6grh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.Request  for  50  observations  from  the  environment,  compute  and  print  the total reward."
      ],
      "metadata": {
        "id": "_yGWSMQz6nXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = MultiArmedBanditEnv()\n",
        "total_reward = 0\n",
        "\n",
        "for _ in range(50):\n",
        "  time_step = env.reset()\n",
        "  action = optimal_policy(time_step.observation)\n",
        "  time_step = env.step(action)\n",
        "  total_reward += time_step.reward\n",
        "\n",
        "print('Total reward:', total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SABLzd3o6rGy",
        "outputId": "64527d9b-1088-4976-c480-52da389716ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 118.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2 –Create an environment "
      ],
      "metadata": {
        "id": "lXhkyZLY6viV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized."
      ],
      "metadata": {
        "id": "eKD9RunD60HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self, reward_sign):\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._episode_ended = False\n",
        "    self._observation = None\n",
        "    self._reward = None\n",
        "    self._reward_sign = reward_sign\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._observation = np.random.randint(low=-5, high=6)\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._reward_sign == 'original':\n",
        "      self._reward = self._observation * action\n",
        "    else:\n",
        "      self._reward = -self._observation * action\n",
        "\n",
        "    self._episode_ended = True\n",
        "    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)"
      ],
      "metadata": {
        "id": "5EG822T46wco"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.Define a policy that detects the behaviorof the underlying environment. There are three situations that the policy needs to handle:\n",
        "\n",
        "i.The agent has not detected know yet which version of the environment is running.\n",
        "\n",
        "ii.The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
        "\n",
        "iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running"
      ],
      "metadata": {
        "id": "hc06OOwO7Qij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "  def __init__(self):\n",
        "    self._state = 'unknown'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    if self._state == 'unknown':\n",
        "      if observation >= 0:\n",
        "        self._state = 'original'\n",
        "        return 2\n",
        "      else:\n",
        "        self._state = 'flipped'\n",
        "        return 0\n",
        "    elif self._state == 'original':\n",
        "      return 2\n",
        "    else:\n",
        "      return 0"
      ],
      "metadata": {
        "id": "5Oc0WQwR7Zwt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.Define the agent that detects the sign of the environment and sets the policy appropriately."
      ],
      "metadata": {
        "id": "Ld07mw_C7ek6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self._policy = Policy()\n",
        "\n",
        "  def update_policy(self, reward_sign):\n",
        "    if reward_sign == 'original':\n",
        "      self._policy._state = 'original'\n",
        "    else:\n",
        "      self._policy._state = 'flipped'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    return self._policy.get_action(observation)"
      ],
      "metadata": {
        "id": "vcPSDPi07iTl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y-V3iRsU-97U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example output for each case: Case 1: Original Environment (reward = observation * action)"
      ],
      "metadata": {
        "id": "R-dHLWzX-BkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='original')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('original')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sujfRrFb-FqB",
        "outputId": "eba10ccf-f4b0-4d4a-e96a-b766fce12753"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward:  24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward should be positive since the optimal policy will result in positive rewards for positive observations and negative rewards for negative observations."
      ],
      "metadata": {
        "id": "TCGyasXA-J-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 2: Flipped Environment (reward = -observation * action)"
      ],
      "metadata": {
        "id": "Sd7V5e4F-PKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='flipped')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('flipped')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jeGsYzJ-Tbk",
        "outputId": "31855417-6789-4a7e-e780-6ba23a78b08d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines an environment where the reward is always 0, regardless of the action and observation. Therefore, the total reward in this environment should always be 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "xIGZiwQZ8FDd"
      }
    }
  ]
}