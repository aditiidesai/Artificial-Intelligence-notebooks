{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AI WEEK 6\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Aditi Desai*\n",
        "\n",
        "*200968126*\n"
      ],
      "metadata": {
        "id": "RAjaffR9hu1O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvcH9swTGnIo"
      },
      "source": [
        "MULTI ARMED BANDITS - AD OPTIMIZATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1sFqZLog5tn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI6LnXRipTvh"
      },
      "outputs": [],
      "source": [
        "# Dataset import\n",
        "dataset = pd.read_csv('Ads_Optimisation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_GsF3FGG1J-"
      },
      "source": [
        "A. Write down the MAB agent problem formulation in your own words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY5fp9aWJYCz"
      },
      "source": [
        "MAB agent Formulation\n",
        "\n",
        "It is a reinforcement learning algorithm that is suited for single-step reinforcement learning. In this situation, the reinforcement learning agent must find an efficient method to find the ad with the highest CTR(Click Through Rate).\n",
        "\n",
        "The reinforcement learning agent must also decide between choosing the best-performing ad and exploring other options. To achieve this, it uses the “epsilon-greedy” (ε-greedy) algorithm. The model will choose the best solution most of the time, and in a specified percent of cases (the epsilon factor), it will select one of the ads at random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZkR6ksZ5mHB"
      },
      "source": [
        "### Random Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqnuiNhhpnIW"
      },
      "outputs": [],
      "source": [
        "# Random Selection\n",
        "import random\n",
        "N = 10000\n",
        "d = 10\n",
        "ads_selected = []\n",
        "total_reward = 0\n",
        "for n in range(0, N):\n",
        "    ad = random.randrange(d)\n",
        "    ads_selected.append(ad)\n",
        "    reward = dataset.values[n, ad]\n",
        "    total_reward = total_reward + reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuwkHUQKpn4u",
        "outputId": "7e0dcc85-eacb-49f3-b2fd-9536030b9561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2    0.112\n",
              "0    0.110\n",
              "7    0.109\n",
              "4    0.107\n",
              "1    0.107\n",
              "5    0.103\n",
              "9    0.095\n",
              "6    0.092\n",
              "3    0.084\n",
              "8    0.081\n",
              "dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.Series(ads_selected).tail(1000).value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-gnXi1lKFYd"
      },
      "source": [
        "B. Compute the total rewards after 2000-time steps using the ε-greedy action for ε=0.01, ε= 0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed1vLJru5ph9"
      },
      "source": [
        "### Epsilon-Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpJth1Wm1JpR",
        "outputId": "d94adf19-fd1f-4eed-e951-f0fa2b0390a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards obtained with epsilon = 0.01: 335.0\n",
            "Total rewards obtained with epsilon = 0.3: 301.0\n"
          ]
        }
      ],
      "source": [
        "np.seterr(invalid='ignore')\n",
        "ads = np.loadtxt('Ads_Optimisation.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "# define the number of arms (ads) and the total number of time steps\n",
        "num_arms = ads.shape[1]\n",
        "num_steps = 2000\n",
        "\n",
        "# initialize the number of times each arm was pulled and the total rewards obtained\n",
        "num_pulls = np.zeros(num_arms)\n",
        "total_rewards = np.zeros(num_arms)\n",
        "\n",
        "# define the epsilon-greedy action function\n",
        "def epsilon_greedy_action(epsilon):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        # explore: choose a random arm\n",
        "        return np.random.randint(num_arms)\n",
        "    else:\n",
        "        # exploit: choose the arm with the highest estimated reward\n",
        "        return np.argmax(total_rewards / num_pulls)\n",
        "\n",
        "# un the bandit algorithm for each value of epsilon\n",
        "for epsilon in [0.01, 0.3]:\n",
        "\n",
        "    # reset the number of times each arm was pulled and the total rewards obtained\n",
        "    num_pulls = np.zeros(num_arms)\n",
        "    total_rewards = np.zeros(num_arms)\n",
        "\n",
        "    # run the bandit algorithm for the specified number of time steps\n",
        "    for t in range(num_steps):\n",
        "        # choose which arm to pull using the epsilon-greedy action\n",
        "        arm = epsilon_greedy_action(epsilon)\n",
        "        \n",
        "        # update the number of times the chosen arm was pulled and the total rewards obtained\n",
        "        reward = ads[t][arm]\n",
        "        num_pulls[arm] += 1\n",
        "        total_rewards[arm] += reward\n",
        "\n",
        "    # compute the total rewards obtained\n",
        "    print(\"Total rewards obtained with epsilon = {}: {}\".format(epsilon, np.sum(total_rewards)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dXy-oWh3C5B"
      },
      "source": [
        "Choosing a higher value of epsilon can lead to a higher total reward. However, this approach may not be optimal in the long run, as it can lead to suboptimal decisions when the algorithm becomes too focused on exploration and does not exploit the highest-reward arm enough.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROoiIfF7Ldjc"
      },
      "source": [
        "C. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c= 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9LNoU75tOe"
      },
      "source": [
        "### Upper-Confidence-Bound action method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Ckn2Pf385E",
        "outputId": "6f65c99d-009d-40ed-e71d-050ea8423517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards obtained with UCB algorithm (c = 1.5): 319.0\n"
          ]
        }
      ],
      "source": [
        "# initialize the number of times each arm was pulled and the total rewards obtained\n",
        "num_pulls = np.zeros(num_arms)\n",
        "total_rewards = np.zeros(num_arms)\n",
        "\n",
        "# run the UCB algorithm with the specified value of c\n",
        "c = 1.5\n",
        "for t in range(num_steps):\n",
        "    # Choose which arm to pull using the UCB action\n",
        "    if t < num_arms:\n",
        "        arm = t  # Choose each arm once in the first num_arms steps\n",
        "    else:\n",
        "        ucb_values = total_rewards / num_pulls + c * np.sqrt(np.log(t) / num_pulls)\n",
        "        arm = np.argmax(ucb_values)\n",
        "        \n",
        "    # Update the number of times the chosen arm was pulled and the total rewards obtained\n",
        "    reward = ads[t][arm]\n",
        "    num_pulls[arm] += 1\n",
        "    total_rewards[arm] += reward\n",
        "\n",
        "# Compute the total rewards obtained\n",
        "print(\"Total rewards obtained with UCB algorithm (c = 1.5): {}\".format(np.sum(total_rewards)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPtSPf7v5hkx"
      },
      "source": [
        "In UCB, the algorithm tends to explore less often as it becomes more confident about the rewards of each arm. Because of this it may miss out on better rewards that could be obtained by exploring more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv-5f0ggLp75"
      },
      "source": [
        "D. For all approaches, explain how the action value estimated compares to the optimal  action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdOO4HX16BNL"
      },
      "source": [
        "Action value vs Optimal action\n",
        "\n",
        "- **Epsilon-greedy approach:** The action value estimated is the sample average of the rewards obtained for each action. The optimal action is the action that has the highest estimated action value. However, since the epsilon-greedy approach explores with a fixed probability, it may sometimes choose a suboptimal action, even if it has a lower estimated action value.\n",
        "\n",
        "- **UCB approach:** The action value estimated is based on both the sample average of the rewards obtained for each action and the uncertainty or confidence in that estimate. The UCB approach tries to balance the exploration and exploitation trade-off by choosing actions that have high estimated action values as well as high uncertainty. The optimal action is the action that has the highest estimated action value with the highest uncertainty or confidence."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}